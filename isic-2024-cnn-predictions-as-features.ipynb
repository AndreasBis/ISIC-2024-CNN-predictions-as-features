{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":8982084,"sourceType":"datasetVersion","datasetId":5406640},{"sourceId":8991790,"sourceType":"datasetVersion","datasetId":5415918},{"sourceId":186147615,"sourceType":"kernelVersion"},{"sourceId":186149019,"sourceType":"kernelVersion"},{"sourceId":187730674,"sourceType":"kernelVersion"},{"sourceId":188543089,"sourceType":"kernelVersion"},{"sourceId":188543756,"sourceType":"kernelVersion"},{"sourceId":188602899,"sourceType":"kernelVersion"},{"sourceId":188603204,"sourceType":"kernelVersion"},{"sourceId":188603902,"sourceType":"kernelVersion"},{"sourceId":190801726,"sourceType":"kernelVersion"},{"sourceId":190815443,"sourceType":"kernelVersion"},{"sourceId":190817588,"sourceType":"kernelVersion"},{"sourceId":193364151,"sourceType":"kernelVersion"},{"sourceId":194167012,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1169.018945,"end_time":"2024-09-04T16:49:58.088413","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-04T16:30:29.069468","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<table style=\"border-collapse: collapse; width: 100%;\">\n  <thead>\n    <tr>\n      <th style=\"padding: 8px; text-align: center; background-color: #8E716C; color: white;\">Model Name</th>\n      <th style=\"padding: 8px; text-align: center; background-color: #8E716C; color: white;\">Notebook Version</th>\n      <th style=\"padding: 8px; text-align: center; background-color: #8E716C; color: white;\">1st best CV</th>\n      <th style=\"padding: 8px; text-align: center; background-color: #8E716C; color: white;\">2nd best CV</th>\n      <th style=\"padding: 8px; text-align: center; background-color: #8E716C; color: white;\">3rd best CV</th>\n      <th style=\"padding: 8px; text-align: center; background-color: #8E716C; color: white;\">Public LB</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr style=\"background-color: #B08E86; color: white;\">\n      <td style=\"padding: 8px; text-align: center;\">EffNetV1B0</td>\n      <td style=\"padding: 8px; text-align: center;\">27</td>\n      <td style=\"padding: 8px; text-align: center;\">0.186</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1857</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1852</td>\n      <td style=\"padding: 8px; text-align: center;\">0.177</td>\n    </tr>\n    <tr style=\"background-color: #C8AAA3; color: white;\">\n      <td style=\"padding: 8px; text-align: center;\">Eva02</td>\n      <td style=\"padding: 8px; text-align: center;\">28</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1844</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1843</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1837</td>\n      <td style=\"padding: 8px; text-align: center;\">0.174</td>\n    </tr>\n    <tr style=\"background-color: #B08E86; color: white;\">\n      <td style=\"padding: 8px; text-align: center;\">Image3</td>\n      <td style=\"padding: 8px; text-align: center;\">29</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1821</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1809</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1809</td>\n      <td style=\"padding: 8px; text-align: center;\">0.176</td>\n    </tr>\n    <tr style=\"background-color: #C8AAA3; color: white;\">\n      <td style=\"padding: 8px; text-align: center;\">EdgeNext</td>\n      <td style=\"padding: 8px; text-align: center;\">30</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1898</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1897</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1896</td>\n      <td style=\"padding: 8px; text-align: center;\">0.176</td>\n    </tr>\n    <tr style=\"background-color: #B08E86; color: white;\">\n      <td style=\"padding: 8px; text-align: center;\">ImageNet</td>\n      <td style=\"padding: 8px; text-align: center;\">31</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1748</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1748</td>\n      <td style=\"padding: 8px; text-align: center;\">0.1744</td>\n      <td style=\"padding: 8px; text-align: center;\">0.178</td>\n    </tr>\n  </tbody>\n</table>","metadata":{"papermill":{"duration":0.015919,"end_time":"2024-09-04T16:30:31.965247","exception":false,"start_time":"2024-09-04T16:30:31.949328","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 50px; font-weight: bold;'>Imports</span></b>","metadata":{"papermill":{"duration":0.014784,"end_time":"2024-09-04T16:30:31.995152","exception":false,"start_time":"2024-09-04T16:30:31.980368","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Handle warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":0.030275,"end_time":"2024-09-04T16:30:32.040456","exception":false,"start_time":"2024-09-04T16:30:32.010181","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data preprocessing\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nfrom pathlib import Path","metadata":{"papermill":{"duration":1.029888,"end_time":"2024-09-04T16:30:33.085418","exception":false,"start_time":"2024-09-04T16:30:32.055530","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploratory data analysis\nimport plotly.express as px\nimport plotly.graph_objects as go","metadata":{"papermill":{"duration":0.642128,"end_time":"2024-09-04T16:30:33.742680","exception":false,"start_time":"2024-09-04T16:30:33.100552","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix","metadata":{"papermill":{"duration":1.213708,"end_time":"2024-09-04T16:30:34.971635","exception":false,"start_time":"2024-09-04T16:30:33.757927","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model development\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import GroupKFold","metadata":{"papermill":{"duration":3.835908,"end_time":"2024-09-04T16:30:38.822918","exception":false,"start_time":"2024-09-04T16:30:34.987010","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 50px; font-weight: bold;'>Configuration</span></b>","metadata":{"papermill":{"duration":0.015237,"end_time":"2024-09-04T16:30:38.853801","exception":false,"start_time":"2024-09-04T16:30:38.838564","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    \n    # Paths to competition data\n    train_path = Path('/kaggle/input/isic-2024-challenge/train-metadata.csv') \n    test_path = Path('/kaggle/input/isic-2024-challenge/test-metadata.csv') \n    subm_path = Path('/kaggle/input/isic-2024-challenge/sample_submission.csv') \n    \n    # Feature engineering arguments\n    N = 39300\n    batch_size = 131072\n    \n    # First model\n    use_cnn1 = False\n    tr1_path = Path('/kaggle/input/isic-inference-effnetv1b0-for-training-data/train_effnetv1b0.csv')\n    te1_path = Path('submission_effnetv1b0.csv')\n    \n    # Second model\n    use_cnn2 = False\n    tr2_path = Path('/kaggle/input/isic-inference-eva02-for-training-data/train_eva02.csv')\n    te2_path = Path('submission_eva02.csv')\n    \n    # Third model\n    use_cnn3 = False\n    tr3_path = Path('/kaggle/input/isic-2024-pl-submission-script-and-preds/train_preds.csv')\n    te3_path = Path('submission_image3.csv')\n    \n    # Fourth model\n    use_cnn4 = False\n    tr4_path = Path('/kaggle/input/isic-inference-edgenext-for-training-data/train_edgenext.csv')\n    te4_path = Path('submission_edgenext.csv')\n    \n    # Fifth model\n    use_cnn5 = True\n    tr5_path = Path('/kaggle/input/isic-2024-imagenet-lr-ramp-target-mods/v5oof_predictions.csv')\n    te5_path = Path('submission_imagenet.csv')\n    \n    # Model development arguments\n    colorscale = 'Redor'\n    early_stop = 100\n    top_models = 3\n    \n    # LightGBM weight and parameters\n    lgb_w = 0.28\n    lgb_p = {\n        'min_child_samples': 48,\n        'num_iterations': 6000,\n        'learning_rate': 0.03,\n        'objective': 'binary',\n        'extra_trees': True,\n        'metric': 'binary',\n        'reg_lambda': 0.8,\n        'reg_alpha': 0.1,\n        'num_leaves': 64,\n        'device': 'cpu',\n        'max_bin': 128,\n        'max_depth': 4,\n        'verbose': -1,\n        'seed': 42\n    }\n    \n    # CatBoost weight and parameters\n    ctb_w = 0.72\n    ctb_p = {\n        'grow_policy': 'Depthwise',\n        'loss_function': 'Logloss',\n        'min_child_samples': 48,\n        'learning_rate': 0.03,\n        'random_state': 42,\n        'task_type': 'CPU',\n        'reg_lambda': 0.8,\n        'num_trees': 6000,\n        'depth': 4\n    }","metadata":{"papermill":{"duration":0.030059,"end_time":"2024-09-04T16:30:38.899097","exception":false,"start_time":"2024-09-04T16:30:38.869038","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 50px; font-weight: bold;'>Infer image models</span></b>","metadata":{"papermill":{"duration":0.015241,"end_time":"2024-09-04T16:30:38.929628","exception":false,"start_time":"2024-09-04T16:30:38.914387","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Select first model\nif CFG.use_cnn1:\n    \n    # Execute script to generate predictions on test data\n    !python /kaggle/input/isic-script-inference-effnetv1b0-f313ae/main.py /kaggle/input/isic-pytorch-training-baseline-image-only/AUROC0.5171_Loss0.3476_epoch35.bin\n    !mv submission.csv submission_effnetv1b0.csv","metadata":{"_kg_hide-output":true,"papermill":{"duration":14.143679,"end_time":"2024-09-04T16:30:53.088532","exception":false,"start_time":"2024-09-04T16:30:38.944853","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select second  model\nif CFG.use_cnn2:\n    \n    # Execute script to generate predictions on test data\n    !python /kaggle/input/isic-script-inference-eva02/main.py /kaggle/input/isic-pytorch-training-baseline-eva02/AUROC0.5177_Loss0.2829_epoch7.bin\n    !mv submission.csv submission_eva02.csv","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.025592,"end_time":"2024-09-04T16:30:53.130044","exception":false,"start_time":"2024-09-04T16:30:53.104452","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select third model\nif CFG.use_cnn3:\n    \n    # Execute script to generate predictions on test data\n    !python /kaggle/input/isic-2024-pl-submission-script-and-preds/pl_submission.py\n    !mv submission.csv submission_image3.csv","metadata":{"_kg_hide-output":true,"papermill":{"duration":20.752773,"end_time":"2024-09-04T16:31:13.898428","exception":false,"start_time":"2024-09-04T16:30:53.145655","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select fourth model\nif CFG.use_cnn4:\n    \n    # Execute script to generate predictions on test data\n    !python /kaggle/input/isic-script-inference-edgenext/main.py /kaggle/input/isic-pytorch-training-edgenext/Final_model.bin\n    !mv submission.csv submission_edgenext.csv","metadata":{"_kg_hide-output":true,"papermill":{"duration":10.70883,"end_time":"2024-09-04T16:31:24.623275","exception":false,"start_time":"2024-09-04T16:31:13.914445","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select fifth model\nif CFG.use_cnn5:\n    \n    # Execute script to generate predictions on test data\n    !python /kaggle/input/script-inference-imagenet/script.py\n    !mv submission.csv submission_imagenet.csv","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.026393,"end_time":"2024-09-04T16:31:24.666002","exception":false,"start_time":"2024-09-04T16:31:24.639609","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FE:\n    \n    def __init__(self, \n                 N, \n                 batch_size,\n                 use_cnn1,\n                 tr1_path,\n                 te1_path,\n                 use_cnn2,\n                 tr2_path,\n                 te2_path,\n                 use_cnn3,\n                 tr3_path,\n                 te3_path,\n                 use_cnn4,\n                 tr4_path,\n                 te4_path,\n                 use_cnn5,\n                 tr5_path,\n                 te5_path):\n        \n        self.N = N\n        self.batch_size = batch_size\n        self.use_cnn1 = use_cnn1\n        self.tr1_path = tr1_path\n        self.te1_path = te1_path\n        self.use_cnn2 = use_cnn2\n        self.tr2_path = tr2_path\n        self.te2_path = te2_path\n        self.use_cnn3 = use_cnn3\n        self.tr3_path = tr3_path\n        self.te3_path = te3_path\n        self.use_cnn4 = use_cnn4\n        self.tr4_path = tr4_path\n        self.te4_path = te4_path\n        self.use_cnn5 = use_cnn5\n        self.tr5_path = tr5_path\n        self.te5_path = te5_path\n        \n    def filter_data(self, path):\n        \n        # Read dataset as polars DataFrame\n        df = pl.read_csv(path, batch_size=self.batch_size)\n            \n        # Drop redundant columns \n        for col in ['image_type', # Only one unique value on train metadata\n                    'tbp_lv_location_simple', # Similar information to 'tbp_lv_location'\n                    'copyright_license', # Redundant information for lesion classification\n                    \n                    # Included only on train metadata\n                    'lesion_id',\n                    'iddx_full',\n                    'iddx_1',\n                    'iddx_2',\n                    'iddx_3',\n                    'iddx_4',\n                    'iddx_5',\n                    'mel_mitotic_index',\n                    'mel_thick_mm',\n                    'tbp_lv_dnn_lesion_confidence']:\n            \n            if col in df.columns:\n                df = df.drop(col)            \n                    \n        return df \n    \n    def set_datatypes(self, df):\n        \n        # Handle NA values in age approximation column\n        if ('age_approx' in df.columns) and df.select(pl.col('age_approx').str.contains('NA').any()).item():\n            \n            # Replace the value with -1\n            df = df.with_columns(pl.when(pl.col('age_approx') == 'NA').then(-1).otherwise(pl.col('age_approx'))\n                   .alias('age_approx'))\n            \n        # Define numeric columns (int)\n        for col in ['target',\n                    'age_approx',\n                    'tbp_lv_symm_2axis_angle']:\n            \n            # Set dtype for numeric columns (int)\n            if col in df.columns:\n                df = df.with_columns(pl.col(col).cast(pl.Int16))\n                        \n        # Define numeric columns (float)\n        for col in ['clin_size_long_diam_mm', \n                    'tbp_lv_A', \n                    'tbp_lv_Aext', \n                    'tbp_lv_B', \n                    'tbp_lv_Bext', \n                    'tbp_lv_C', \n                    'tbp_lv_Cext',\n                    'tbp_lv_H', \n                    'tbp_lv_Hext',\n                    'tbp_lv_L', \n                    'tbp_lv_Lext',\n                    'tbp_lv_areaMM2', \n                    'tbp_lv_area_perim_ratio',\n                    'tbp_lv_color_std_mean',\n                    'tbp_lv_deltaA', \n                    'tbp_lv_deltaB',\n                    'tbp_lv_deltaL', \n                    'tbp_lv_deltaLB',\n                    'tbp_lv_deltaLBnorm',\n                    'tbp_lv_eccentricity', \n                    'tbp_lv_minorAxisMM',\n                    'tbp_lv_nevi_confidence',\n                    'tbp_lv_norm_border',\n                    'tbp_lv_norm_color',\n                    'tbp_lv_perimeterMM',\n                    'tbp_lv_radial_color_std_max',\n                    'tbp_lv_stdL',\n                    'tbp_lv_stdLExt',\n                    'tbp_lv_symm_2axis',\n                    'tbp_lv_x',\n                    'tbp_lv_y',\n                    'tbp_lv_z']: \n            \n            # Set dtype for numeric columns (float)\n            if col in df.columns:\n                df = df.with_columns(pl.col(col).cast(pl.Float32))\n                \n        # Define categorical columns\n        for col in ['sex', \n                    'anatom_site_general', \n                    'tbp_tile_type', \n                    'tbp_lv_location',\n                    'attribution']:\n            \n            # Set dtype for categorical columns\n            if col in df.columns:\n                df = df.with_columns(pl.col(col).cast(pl.Categorical))\n                                            \n        return df\n    \n    def aggregate_data(self, df):\n\n        df = df.with_columns([\n\n            # Ratio between the smallest and largest diameter of the lesion\n            pl.col('tbp_lv_minorAxisMM')\n            .truediv(pl.col('clin_size_long_diam_mm'))\n            .cast(pl.Float32).alias('tbp_lv_diam_ratio'),\n\n            # Absolute difference between the hue (color tone) inside and outside the lesion\n            (pl.col('tbp_lv_H')\n            .sub(pl.col('tbp_lv_Hext'))).abs()\n            .cast(pl.Float32).alias('tbp_lv_H_contrast'),\n\n            # Absolute difference in lightness (L) between the inside and outside of the lesion\n            (pl.col('tbp_lv_L')\n            .sub(pl.col('tbp_lv_Lext'))).abs()\n            .cast(pl.Float32).alias('tbp_lv_L_contrast'),\n\n            # Euclidean distance combining the differences in the L*A*B* color channels\n            (pl.col('tbp_lv_deltaA')\n            .add(pl.col('tbp_lv_deltaB'))\n            .add(pl.col('tbp_lv_deltaL')))\n            .sqrt()\n            .cast(pl.Float32).alias('tbp_lv_vector_deltaLAB'),\n\n            # Shape complexity index, calculated as the ratio of lesion area to the square of its perimeter\n            pl.col('tbp_lv_areaMM2')\n            .truediv(pl.col('tbp_lv_perimeterMM').pow(2))\n            .cast(pl.Float32).alias('tbp_lv_shape_index'),\n\n            # Ratio of lesion area to perimeter\n            pl.col('tbp_lv_areaMM2')\n            .truediv(pl.col('tbp_lv_perimeterMM'))\n            .cast(pl.Float32).alias('tbp_lv_ratio_area_perim'),\n\n            # Ratio of lesion perimeter to area\n            pl.col('tbp_lv_perimeterMM')\n            .truediv(pl.col('tbp_lv_areaMM2'))\n            .cast(pl.Float32).alias('tbp_lv_ratio_perim_area'),\n\n            # Combined score of border irregularity and asymmetry\n            pl.col('tbp_lv_norm_border')\n            .add(pl.col('tbp_lv_symm_2axis'))\n            .cast(pl.Float32).alias('border_complexity'),\n\n            # Measure of color distribution uniformity, calculated as the ratio of color irregularity to asymmetry\n            pl.col('tbp_lv_color_std_mean')\n            .truediv(pl.col('tbp_lv_radial_color_std_max')\n                     .add(pl.lit(1e-6)))  # Adding a small constant to avoid division by zero\n            .cast(pl.Float32).alias('color_uniformity'),\n\n            # 3D Euclidean distance of the lesion's position, based on its (x, y, z) coordinates\n            (pl.col('tbp_lv_x').pow(2)\n             .add(pl.col('tbp_lv_y').pow(2))\n             .add(pl.col('tbp_lv_z').pow(2)))\n            .sqrt()\n            .cast(pl.Float32).alias('position_distance_3d'),\n\n            # Visibility score combining lesion contrast and normalized color metrics\n            pl.col('tbp_lv_deltaLBnorm')\n            .add(pl.col('tbp_lv_norm_color'))\n            .cast(pl.Float32).alias('lesion_visibility_score'),\n\n            # Concatenation of the general anatomical site and the lesion's specific location\n            pl.col('anatom_site_general')\n            .add('_')\n            .add(pl.col('tbp_lv_location'))\n            .cast(pl.Categorical).alias('combined_anatomical_site'),\n\n            # Product of symmetry and border irregularity scores\n            pl.col('tbp_lv_symm_2axis')\n            .mul(pl.col('tbp_lv_norm_border'))\n            .cast(pl.Float32).alias('symmetry_border_consistency'),\n\n            # Ratio of the product of symmetry and border irregularity to their sum\n            pl.col('tbp_lv_symm_2axis')\n            .mul(pl.col('tbp_lv_norm_border'))\n            .truediv(pl.col('tbp_lv_symm_2axis')\n                     .add(pl.col('tbp_lv_norm_border')))\n            .cast(pl.Float32).alias('consistency_symmetry_border'),\n\n            # Ratio of standard deviation of lightness inside the lesion to the external lightness\n            pl.col('tbp_lv_stdL')\n            .truediv(pl.col('tbp_lv_Lext'))\n            .cast(pl.Float32).alias('color_consistency'),\n\n            # Combination of internal and external lightness variability, normalized to their sum\n            pl.col('tbp_lv_stdL')\n            .mul(pl.col('tbp_lv_Lext'))\n            .truediv(pl.col('tbp_lv_stdL')\n                     .add(pl.col('tbp_lv_Lext')))\n            .cast(pl.Float32).alias('consistency_color'),\n\n            # Interaction term between the lesion's size and the patient's age\n            pl.col('clin_size_long_diam_mm')\n            .mul(pl.col('age_approx'))\n            .cast(pl.Float32).alias('size_age_interaction'),\n\n            # Interaction between the lesion's hue and color variance\n            pl.col('tbp_lv_H')\n            .mul(pl.col('tbp_lv_color_std_mean'))\n            .cast(pl.Float32).alias('hue_color_std_interaction'),\n\n            # Composite index for lesion severity, averaging border irregularity, color variation, and eccentricity\n            (pl.col('tbp_lv_norm_border')\n             .add(pl.col('tbp_lv_norm_color'))\n             .add(pl.col('tbp_lv_eccentricity')))\n            .truediv(3)\n            .cast(pl.Float32).alias('lesion_severity_index'),\n\n            ])\n\n        df = df.with_columns([\n\n            # Combined index of shape complexity, including both border complexity and shape index\n            pl.col('border_complexity')\n            .add(pl.col('tbp_lv_shape_index'))\n            .cast(pl.Float32).alias('shape_complexity_index'),\n\n            # Composite color contrast index, summing the differences in L*A*B* channels and the contrast with surrounding skin\n            pl.col('tbp_lv_deltaA')\n            .add(pl.col('tbp_lv_deltaB'))\n            .add(pl.col('tbp_lv_deltaL'))\n            .add(pl.col('tbp_lv_deltaLBnorm'))\n            .cast(pl.Float32).alias('color_contrast_index'),\n\n            # Logarithmic transformation of lesion area, to reduce the skewness of area values\n            pl.col('tbp_lv_areaMM2').log1p()\n            .cast(pl.Float32).alias('log_lesion_area'),\n\n            # Normalized lesion size by dividing the long diameter by the patient's approximate age\n            pl.col('clin_size_long_diam_mm')\n            .truediv(pl.col('age_approx'))\n            .cast(pl.Float32).alias('normalized_lesion_size'),\n\n            # Mean of the hue inside and outside the lesion.\n            (pl.col('tbp_lv_H')\n             .add(pl.col('tbp_lv_Hext')))\n            .truediv(2)\n            .cast(pl.Float32).alias('mean_hue_difference'),\n\n            # Standard deviation of contrast across L*A*B* channels, measuring overall color variability\n            ((pl.col('tbp_lv_deltaA').pow(2)\n             .add(pl.col('tbp_lv_deltaB').pow(2))\n             .add(pl.col('tbp_lv_deltaL').pow(2)))\n            .truediv(3))\n            .sqrt()\n            .cast(pl.Float32).alias('std_dev_contrast'),\n\n            # Composite index combining color irregularity, area-to-perimeter ratio, and symmetry\n            (pl.col('tbp_lv_color_std_mean')\n             .add(pl.col('tbp_lv_area_perim_ratio'))\n             .add(pl.col('tbp_lv_symm_2axis')))\n            .truediv(3)\n            .cast(pl.Float32).alias('color_shape_composite_index'),\n\n            # 3D orientation of the lesion, calculated using the arctangent of its Y and X coordinates\n            pl.arctan2(pl.col('tbp_lv_y'), \n                       pl.col('tbp_lv_x'))\n            .cast(pl.Float32).alias('lesion_orientation_3d'),\n\n            # Mean color difference across the L*A*B* channels, providing an overall color difference score\n            (pl.col('tbp_lv_deltaA')\n             .add(pl.col('tbp_lv_deltaB'))\n             .add(pl.col('tbp_lv_deltaL')))\n            .truediv(3)\n            .cast(pl.Float32).alias('overall_color_difference'),\n\n            # Interaction between symmetry and perimeter.\n            pl.col('tbp_lv_symm_2axis')\n            .mul(pl.col('tbp_lv_perimeterMM'))\n            .cast(pl.Float32).alias('symmetry_perimeter_interaction'),\n\n            # Average of area-perimeter ratio, eccentricity, color irregularity, and symmetry\n            (pl.col('tbp_lv_area_perim_ratio')\n             .add(pl.col('tbp_lv_eccentricity'))\n             .add(pl.col('tbp_lv_norm_color'))\n             .add(pl.col('tbp_lv_symm_2axis')))\n            .truediv(4)\n            .cast(pl.Float32).alias('comprehensive_lesion_index'),\n\n            # Ratio of internal color variance to external color standard deviation\n            pl.col('tbp_lv_color_std_mean')\n            .truediv(pl.col('tbp_lv_stdLExt'))\n            .cast(pl.Float32).alias('color_variance_ratio'),\n\n            # Interaction between border irregularity and color irregularity\n            pl.col('tbp_lv_norm_border')\n            .mul(pl.col('tbp_lv_norm_color'))\n            .cast(pl.Float32).alias('border_color_interaction'),\n\n            # Normalized interaction between border irregularity and color irregularity\n            pl.col('tbp_lv_norm_border')\n            .mul(pl.col('tbp_lv_norm_color'))\n            .truediv(pl.col('tbp_lv_norm_border')\n                     .add(pl.col('tbp_lv_norm_color')))\n            .cast(pl.Float32).alias('border_color_interaction_2'),\n\n            # Ratio of lesion size to contrast with the surrounding skin\n            pl.col('clin_size_long_diam_mm')\n            .truediv(pl.col('tbp_lv_deltaLBnorm'))\n            .cast(pl.Float32).alias('size_color_contrast_ratio'),\n\n            # Nevus confidence score normalized by patient age\n            pl.col('tbp_lv_nevi_confidence')\n            .truediv(pl.col('age_approx'))\n            .cast(pl.Float32).alias('age_normalized_nevi_confidence'),\n\n            # Geometric mean of lesion size and age\n            (pl.col('clin_size_long_diam_mm').pow(2)\n             .add(pl.col('age_approx').pow(2)))\n            .sqrt()\n            .cast(pl.Float32).alias('age_normalized_nevi_confidence_2'),\n\n            # Interaction between color asymmetry and symmetry\n            pl.col('tbp_lv_radial_color_std_max')\n            .mul(pl.col('tbp_lv_symm_2axis'))\n            .cast(pl.Float32).alias('color_asymmetry_index'),\n\n            # Approximation of lesion volume in 3D space\n            pl.col('tbp_lv_areaMM2')\n            .mul((pl.col('tbp_lv_x').pow(2)\n                  .add(pl.col('tbp_lv_y').pow(2))\n                  .add(pl.col('tbp_lv_z').pow(2)))\n                 .sqrt())\n            .cast(pl.Float32).alias('volume_approximation_3d'),\n\n            # Sum of absolute differences between the L*A*B* channels inside and outside the lesion\n            (pl.col('tbp_lv_L')\n             .sub(pl.col('tbp_lv_Lext'))).abs()\n            .add((pl.col('tbp_lv_A')\n                  .sub(pl.col('tbp_lv_Aext'))).abs())\n            .add((pl.col('tbp_lv_B')\n                  .sub(pl.col('tbp_lv_Bext'))).abs())\n            .cast(pl.Float32).alias('color_range'),\n\n            # Interaction between lesion eccentricity and color irregularity\n            pl.col('tbp_lv_eccentricity')\n            .mul(pl.col('tbp_lv_color_std_mean'))\n            .cast(pl.Float32).alias('shape_color_consistency'),\n\n            # Border length ratio, calculated as the ratio of the lesion's perimeter to a perfect circle's perimeter with the same area\n            pl.col('tbp_lv_perimeterMM')\n            .truediv(pl.lit(2)\n                     .mul(np.pi)\n                     .mul((pl.col('tbp_lv_areaMM2')\n                           .truediv(np.pi))\n                          .sqrt()))\n            .cast(pl.Float32).alias('border_length_ratio'),\n\n            # Composite index combining age, lesion size, and symmetry, indicating the interaction between these factors\n            pl.col('age_approx')\n            .mul(pl.col('clin_size_long_diam_mm'))\n            .mul(pl.col('tbp_lv_symm_2axis'))\n            .cast(pl.Float32).alias('age_size_symmetry_index'),\n\n            # Alternative composite index combining age, lesion area, and symmetry\n            pl.col('age_approx')\n            .mul(pl.col('tbp_lv_areaMM2'))\n            .mul(pl.col('tbp_lv_symm_2axis'))\n            .cast(pl.Float32).alias('index_age_size_symmetry'),\n\n            # Count of lesions per patient\n            pl.col('isic_id').count()\n            .over('patient_id')\n            .cast(pl.Int16).alias('tbp_lv_count'),\n\n        ])\n\n        # Aggregate all numeric (float) columns using the z-score\n        df = df.with_columns([\n            pl.col(col).sub(pl.col(col).mean()).truediv(pl.col(col).std()).over('patient_id')\n            .cast(pl.Float32).alias(f'{col}_zscore') for col in df.columns if df[col].dtype == pl.Float32\n        ])\n\n        return df\n    \n    def extract_cat_cols(self, df):\n        \n        # Define a list of categorical columns\n        cat_cols = []\n        \n        # Find categorical columns\n        for col in df.columns:\n            if df[col].dtype == pl.Categorical:\n                cat_cols.append(col)\n                \n        return cat_cols\n    \n    def add_cnn_preds(self, df):\n\n        # Select first model\n        if self.use_cnn1:\n\n            if 'target' in df.columns:\n\n                # Load train predictions of the first model\n                tr1 = pl.read_csv(self.tr1_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(tr1.select(pl.col('target_effnetv1b0').cast(pl.Float32).alias('target_effnetv1b0')))\n\n            else:\n\n                # Load test predictions of the first model\n                te1 = pl.read_csv(self.te1_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(te1.select(pl.col('target').cast(pl.Float32).alias('target_effnetv1b0')))\n\n        # Select second model\n        if self.use_cnn2:\n\n            if 'target' in df.columns:\n\n                # Load train predictions of the second model\n                tr2 = pl.read_csv(self.tr2_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(tr2.select(pl.col('target_eva02').cast(pl.Float32).alias('target_eva02')))\n\n            else:\n\n                # Load test predictions of the second model\n                te2 = pl.read_csv(self.te2_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(te2.select(pl.col('target').cast(pl.Float32).alias('target_eva02')))\n\n        # Select third model\n        if self.use_cnn3:\n\n            if 'target' in df.columns:\n\n                # Load train predictions of the third model\n                tr3 = pl.read_csv(self.tr3_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(tr3.select(pl.col('pred').cast(pl.Float32).alias('target_3')))\n\n            else:\n\n                # Load test predictions of the third model\n                te3 = pl.read_csv(self.te3_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(te3.select(pl.col('target').cast(pl.Float32).alias('target_3')))\n\n        # Select fourth model\n        if self.use_cnn4:\n\n            if 'target' in df.columns:\n\n                # Load train predictions of the fourth model\n                tr4 = pl.read_csv(self.tr4_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(tr4.select(pl.col('target_edgenext').cast(pl.Float32).alias('target_edgenext')))\n\n            else:\n\n                # Load test predictions of the fourth model\n                te4 = pl.read_csv(self.te4_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(te4.select(pl.col('target').cast(pl.Float32).alias('target_edgenext')))\n\n        # Select fifth model\n        if self.use_cnn5:\n\n            if 'target' in df.columns:\n\n                # Load train predictions of the fifth model\n                tr5 = pl.read_csv(self.tr5_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(tr5.select(pl.col('oof_prediction').cast(pl.Float32).alias('oof_prediction')))\n\n            else:\n\n                # Load test predictions of the fifth model\n                te5 = pl.read_csv(self.te5_path, batch_size=self.batch_size)\n\n                # Assign predictions to the dataset\n                df = df.with_columns(te5.select(pl.col('target').cast(pl.Float32).alias('oof_prediction')))\n\n        return df\n    \n    def select_benign_cases(self, df):\n        \n        # Ensure downsampling only works for train data (which include the label column)\n        if 'target' in df.columns:\n    \n            # Extract the counts of positive and negative cases\n            p_cases = df[df['target'] == 1]\n            n_cases = df[df['target'] == 0]\n\n            # Select N negative cases, for initial dataset\n            n_cases = n_cases.sample(n=self.N * 10, random_state=42)\n\n            # Concatenate reduced negative cases with positive cases\n            df = pd.concat([n_cases, p_cases])\n\n        return df\n    \n    def display_info(self, df):\n        \n        # Display the shape of the DataFrame\n        print(f'Shape: {df.shape}')\n            \n        # Display count of unique patients\n        count = df['patient_id'].nunique()\n        print(f'Unique patients: {count}')\n        \n        # Display the memory usage of the DataFrame\n        mem = df.memory_usage().sum() / 1024**2\n        print('Memory usage: {:.2f} MB\\n'.format(mem))\n    \n    def process_data(self, path):\n        \n        # Load and clean dataset\n        df = self.filter_data(path)\n        \n        # Set proper datatypes\n        df = self.set_datatypes(df)\n        \n        # Aggregate dataset\n        df = self.aggregate_data(df)\n            \n        # Extract categorical columns\n        cat_cols = self.extract_cat_cols(df)\n        \n        # Add CNN generated predictions to the DataFrame\n        df = self.add_cnn_preds(df)\n        \n        # Convert to pandas DataFrame\n        df = df.to_pandas()\n            \n        # Downsample benign cases\n        df = self.select_benign_cases(df)\n\n        # Reset DataFrame indices\n        df = df.reset_index(drop=True)\n\n        # Display info about DataFrame\n        self.display_info(df)\n        \n        return df, cat_cols\n    \n    def downsample_data(self, df, index):\n\n        # Separate the malignant (positive) and benign (negative) cases\n        p_cases = df[df['target'] == 1] \n        n_cases = df[df['target'] == 0]\n            \n        # Raise error if the index is invalid\n        if not (1 <= index <= 10):\n            raise ValueError('Index must be an integer between 1 and 10!')\n            \n        # Define start and finish indices for downsampling\n        start = (index - 1) * self.N\n        finish = index * self.N\n        \n        # Select N unique benign (negative) cases for training dataset\n        n_cases = n_cases.iloc[start:finish]\n\n        # Concatenated selected negative cases with all positive cases\n        df = pd.concat([p_cases, n_cases], axis=0)\n        \n        # Reset the indices\n        df = df.reset_index(drop=True)\n        \n        # Display info about DataFrame\n        self.display_info(df)\n        \n        return df","metadata":{"papermill":{"duration":0.097195,"end_time":"2024-09-04T16:31:24.779411","exception":false,"start_time":"2024-09-04T16:31:24.682216","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize class for feature engineering\nfe = FE(CFG.N, \n        CFG.batch_size,\n        CFG.use_cnn1,\n        CFG.tr1_path,\n        CFG.te1_path,\n        CFG.use_cnn2,\n        CFG.tr2_path,\n        CFG.te2_path,\n        CFG.use_cnn3,\n        CFG.tr3_path,\n        CFG.te3_path,\n        CFG.use_cnn4,\n        CFG.tr4_path,\n        CFG.te4_path,\n        CFG.use_cnn5,\n        CFG.tr5_path,\n        CFG.te5_path)","metadata":{"papermill":{"duration":0.025124,"end_time":"2024-09-04T16:31:24.820963","exception":false,"start_time":"2024-09-04T16:31:24.795839","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Metrics:\n    \n    @staticmethod\n    def pauc(y_true, y_scores, tpr_threshold=0.8):\n        \n        # Rescale labels: set 0s to 1s and 1s to 0s (because sklearn only has max_fpr, not min_tpr)\n        rescaled_labels = abs(np.asarray(y_true) - 1)\n\n        # Flip the prediction scores to their complements (to work with rescaled label)\n        flipped_preds = -1.0 * np.asarray(y_scores)\n\n        # Calculate the maximum false positive rate based on the given TPR threshold\n        max_fpr = abs(1 - tpr_threshold)\n\n        # Calculate the ROC curve\n        fpr, tpr, _ = roc_curve(rescaled_labels, flipped_preds, sample_weight=None)\n\n        # Find the index where FPR exceeds max_fpr\n        interp_idx = np.searchsorted(fpr, max_fpr, 'right')\n\n        # Define points for linear interpolation\n        x_interp = [fpr[interp_idx - 1], fpr[interp_idx]]\n        y_interp = [tpr[interp_idx - 1], tpr[interp_idx]]\n\n        # Add interpolated point to TPR and FPR arrays\n        tpr = np.append(tpr[:interp_idx], np.interp(max_fpr, x_interp, y_interp))\n        fpr = np.append(fpr[:interp_idx], max_fpr)\n\n        # Calculate the partial AUC\n        partial_auc = auc(fpr, tpr)\n        \n        return partial_auc\n    \n    @staticmethod\n    def plot_cv(fold_scores, model_name):\n        \n        # Round the fold scores to 4 decimal places\n        fold_scores = [round(score, 4) for score in fold_scores]\n        mean_score = round(np.mean(fold_scores), 4)\n        std_score = round(np.std(fold_scores), 4)\n\n        # Create a new figure for plotting\n        fig = go.Figure()\n\n        # Add scatter plot for individual fold scores\n        fig.add_trace(go.Scatter(\n            x = list(range(1, len(fold_scores) + 1)),\n            y = fold_scores,\n            mode = 'markers', \n            name = 'Fold Scores',\n            marker = dict(size = 24, color='#93C572', symbol='diamond'),  # Diamond shape marker, colored Pistachio\n            text = [f'{score:.4f}' for score in fold_scores],\n            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n            hoverlabel=dict(font=dict(size=16))  # Adjust the font size here\n        ))\n\n        # Add a horizontal line for the mean score\n        fig.add_trace(go.Scatter(\n            x = [1, len(fold_scores)],\n            y = [mean_score, mean_score],\n            mode = 'lines',\n            name = f'Mean: {mean_score:.4f}',\n            line = dict(dash = 'dash', color = '#F08000'), # Colored Tangerine\n            hoverinfo = 'none'\n        ))\n\n        # Update the layout of the plot\n        fig.update_layout(\n            title = f'{model_name} Cross-Validation pAUC Scores | Variation of CV scores: {mean_score} ± {std_score}',\n            xaxis_title = 'Fold',\n            yaxis_title = 'pAUC Score',\n            plot_bgcolor = 'rgba(0,0,0,0)',\n            paper_bgcolor = 'rgba(0,0,0,0)',\n            xaxis = dict(\n                gridcolor = 'lightgray',\n                tickmode = 'linear',\n                tick0 = 1,\n                dtick = 1,\n                range = [0.5, len(fold_scores) + 0.5]\n            ),\n            yaxis = dict(gridcolor = 'lightgray')\n        )\n\n        # Display the plot\n        fig.show() \n        \n    @staticmethod\n    def plot_cm(y_true, y_pred, colorscale, valid_pauc):\n        \n        # Get unique labels\n        labels = sorted(np.unique(y_true))\n        \n        # Compute ROC curve to find the TPR80 threshold\n        _, tpr, thresholds = roc_curve(y_true, y_pred)\n\n        # Find the TRP80 threshold\n        tpr80 = thresholds[np.where(tpr >= 0.8)[0][0]]\n\n        # Compute confusion matrix\n        cm = confusion_matrix(y_true, \n                              y_pred=(y_pred >= tpr80).astype(int), \n                              labels=labels)\n        \n        # Create the heatmap\n        fig = go.Figure(data=go.Heatmap(\n            z=cm,\n            x=labels,\n            y=labels,\n            colorscale=colorscale,\n            zmin=0,\n            \n            # Use the maximum value in the confusion matrix\n            zmax=np.max(cm),  \n            text=cm,\n            texttemplate='%{text:.0f}',\n            hovertemplate='True: %{y}<br>Predicted: %{x}<br>Count: %{z:,.0f}<extra></extra>',\n            \n            # Create a custom hover text format\n            customdata = [str(int(val)) for val in cm.flatten()]\n        ))\n        \n        # Update layout for a transparent background and square aspect ratio\n        fig.update_layout(\n            title = f'Ensemble pAUC score: {round(valid_pauc, 4)} | Confusion Matrix:',\n            plot_bgcolor='rgba(0,0,0,0)',\n            paper_bgcolor='rgba(0,0,0,0)',\n            xaxis_title='Predicted Labels',\n            yaxis_title='True Labels',\n            xaxis=dict(constrain='domain'),\n            yaxis=dict(constrain='domain', scaleanchor='x'),\n            width=900,  \n            height=900,  \n            margin=dict(t=90, b=90, l=90, r=90) \n        )\n        \n        # Show the plot\n        fig.show()","metadata":{"papermill":{"duration":0.040293,"end_time":"2024-09-04T16:31:24.877654","exception":false,"start_time":"2024-09-04T16:31:24.837361","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MD:\n    \n    def __init__(self, \n                 colorscale,\n                 early_stop,\n                 top_models,\n                 lgb_w,\n                 lgb_p,\n                 ctb_w,\n                 ctb_p):\n        \n        self.colorscale = colorscale\n        self.early_stop = early_stop\n        self.top_models = top_models\n        self.lgb_w = lgb_w\n        self.lgb_p = lgb_p\n        self.ctb_w = ctb_w\n        self.ctb_p = ctb_p\n    \n    def train_lgb(self, data, cat_cols, title):\n        \n        # Convert categorical columns to category dtype\n        for col in cat_cols:\n            data[col] = data[col].astype('category')\n        \n        # Split features and label\n        X = data.drop(['target', 'isic_id', 'patient_id'], axis=1)\n        y = data['target']\n        groups = data['patient_id']\n        \n        # Initialize cross validation strategy (GroupKFold)\n        cv = GroupKFold(5)\n        \n        # Initialize lists to store models, cv scores, and OOF predictions\n        models, scores = [], []\n        oof_preds = np.zeros(len(X))\n        \n        # Perform cross-validation\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups)):\n            \n            # Split the data into training and validation sets for the current fold\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n            # Create LightGBM datasets\n            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols)\n            valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_cols, reference=train_data)\n            \n            # Train the model\n            model = lgb.train(self.lgb_p, \n                              train_data, \n                              valid_sets=[valid_data], \n                              callbacks=[lgb.early_stopping(self.early_stop, verbose=0), \n                                         lgb.log_evaluation(0)])\n            \n            # Append the trained model to the list\n            models.append(model)\n            \n            # Make predictions on the validation set\n            oof_preds[valid_index] = model.predict(X_valid)\n            \n            # Calculate and store the pAUC score for the current (valid) fold\n            score = Metrics.pauc(y_valid, oof_preds[valid_index])\n            scores.append(score)\n        \n        # Plot the cross-validation results\n        Metrics.plot_cv(scores, title)\n        \n        return models, oof_preds\n    \n    def train_ctb(self, data, cat_cols, title):\n        \n        # Convert categorical columns to string\n        for col in cat_cols:\n            data[col] = data[col].astype(str)\n        \n        # Split features and label\n        X = data.drop(['target', 'isic_id', 'patient_id'], axis=1)\n        y = data['target']\n        groups = data['patient_id']\n        \n        # Initialize cross validation strategy (GroupKFold)\n        cv = GroupKFold(5)\n        \n        # Initialize lists to store models, cv scores, and OOF predictions\n        models, scores = [], []\n        oof_preds = np.zeros(len(X))\n        \n        # Perform cross-validation\n        for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups)):\n            \n            # Split the data into training and validation sets for the current fold\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n            # Create CatBoost pools\n            train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n            valid_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n            \n            # Initialize CatBoost\n            model = CatBoostClassifier(**self.ctb_p, verbose=0)\n            \n            # Train the model\n            model.fit(train_pool, \n                      eval_set=valid_pool, \n                      early_stopping_rounds=self.early_stop)\n            \n            # Append the trained model to the list\n            models.append(model)\n            \n            # Make predictions on the validation set\n            oof_preds[valid_index] = model.predict_proba(valid_pool)[:, 1]\n            \n            # Calculate and store the pAUC score for the current (valid) fold\n            score = Metrics.pauc(y_valid, oof_preds[valid_index])\n            scores.append(score)\n        \n        # Plot the cross-validation results\n        Metrics.plot_cv(scores, title)\n        \n        return models, oof_preds\n\n    def infer_lgb(self, data, cat_cols, models):\n\n        # Convert categorical columns to category dtype\n        for col in cat_cols:\n            data[col] = data[col].astype('category')\n\n        # Return the averaged predictions of LightGBM classifiers\n        return np.mean([model.predict(data) for model in models], axis=0)\n    \n    def infer_ctb(self, data, cat_cols, models):\n        \n        # Convert categorical columns to string\n        for col in cat_cols:\n            data[col] = data[col].astype(str)\n        \n        # Create CatBoost pool for inference\n        pool = Pool(data, cat_features=cat_cols)\n        \n        # Return the averaged predictions of CatBoost classifiers\n        return np.mean([model.predict_proba(pool)[:, 1] for model in models], axis=0)\n    \n    def generate_preds(self, train_data, test_data, cat_cols):\n        \n        # Extract features columns and label\n        X = train_data.drop(['target', 'isic_id', 'patient_id'], axis=1)\n        y = train_data['target']\n        \n        # Train LightGBM and CatBoost\n        lgb_models, oof_lgb_preds = self.train_lgb(train_data, cat_cols, 'LightGBM')\n        ctb_models, oof_ctb_preds = self.train_ctb(train_data, cat_cols, 'CatBoost')\n        \n        # Blend the out-of-fold (OOF) predictions of LightGBM and CatBoost\n        valid_preds = oof_lgb_preds * self.lgb_w + oof_ctb_preds * self.ctb_w\n        \n        # Calculate pAUC score and plot confusion matrix for out-of-fold (OOF) predictions\n        valid_pauc = Metrics.pauc(y, valid_preds)\n        Metrics.plot_cm(y, valid_preds, self.colorscale, valid_pauc)  \n        \n        # Prepare test data for inference\n        test_data = test_data.drop(['isic_id', 'patient_id'], axis=1)\n\n        # Infer LightGBM and CatBoost on test data\n        test_lgb_preds = self.infer_lgb(test_data, cat_cols, lgb_models)\n        test_ctb_preds = self.infer_ctb(test_data, cat_cols, ctb_models)\n        \n        # Blend LightGBM and CatBoost predictions\n        test_preds = test_lgb_preds * self.lgb_w + test_ctb_preds * self.ctb_w\n        \n        return test_preds, valid_pauc\n    \n    def cherry_pick_models(self, results):\n        \n        # Sort the list of tuples by pAUC score in descending order (best score first)\n        results.sort(key=lambda x: x[1], reverse=True)\n\n        # Select the ensembles with highest validation pAUC score\n        models = results[:self.top_models] \n\n        # Extract predictions and pAUC scores of the ensembles\n        top_preds = [preds for preds, pauc in models]\n        top_scores = [pauc for preds, pauc in models]\n\n        # Round the values of the pAUC scores to 4 digits\n        top_scores = [round(score, 4) for score in top_scores]\n        print('(Ranked) pAUC scores:', *top_scores)\n        \n        # Return top predictions as a NumPy array\n        return np.array(top_preds)","metadata":{"papermill":{"duration":0.04736,"end_time":"2024-09-04T16:31:24.941467","exception":false,"start_time":"2024-09-04T16:31:24.894107","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize class for model development\nmd = MD(CFG.colorscale,\n        CFG.early_stop,\n        CFG.top_models,\n        CFG.lgb_w,\n        CFG.lgb_p,\n        CFG.ctb_w,\n        CFG.ctb_p)","metadata":{"papermill":{"duration":0.024869,"end_time":"2024-09-04T16:31:24.983031","exception":false,"start_time":"2024-09-04T16:31:24.958162","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 50px; font-weight: bold;'>Feature Engineering</span></b>","metadata":{"papermill":{"duration":0.016744,"end_time":"2024-09-04T16:31:25.016729","exception":false,"start_time":"2024-09-04T16:31:24.999985","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load train data and categorical columns\ntrain_data, cat_cols = fe.process_data(CFG.train_path)","metadata":{"papermill":{"duration":5.306325,"end_time":"2024-09-04T16:31:30.394448","exception":false,"start_time":"2024-09-04T16:31:25.088123","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load test data\ntest_data, _ = fe.process_data(CFG.test_path)","metadata":{"papermill":{"duration":0.056162,"end_time":"2024-09-04T16:31:30.469077","exception":false,"start_time":"2024-09-04T16:31:30.412915","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a list to store test predictions and pAUC scores\nresults = []","metadata":{"papermill":{"duration":0.024285,"end_time":"2024-09-04T16:31:30.510672","exception":false,"start_time":"2024-09-04T16:31:30.486387","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 50px; font-weight: bold;'>Model Development</span></b>","metadata":{"papermill":{"duration":0.016618,"end_time":"2024-09-04T16:31:30.544154","exception":false,"start_time":"2024-09-04T16:31:30.527536","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 1<span style=\"vertical-align: super; font-size: 60%;\">st</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.016513,"end_time":"2024-09-04T16:31:30.577496","exception":false,"start_time":"2024-09-04T16:31:30.560983","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 1st training dataset\ntrain_01 = fe.downsample_data(train_data, 1)","metadata":{"papermill":{"duration":0.21019,"end_time":"2024-09-04T16:31:30.804868","exception":false,"start_time":"2024-09-04T16:31:30.594678","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 1st training dataset\npreds_01, pauc_01 = md.generate_preds(train_01, test_data, cat_cols)","metadata":{"papermill":{"duration":109.308625,"end_time":"2024-09-04T16:33:20.130861","exception":false,"start_time":"2024-09-04T16:31:30.822236","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 1st ensemble\nresults.append((preds_01, pauc_01))","metadata":{"papermill":{"duration":0.025936,"end_time":"2024-09-04T16:33:20.175435","exception":false,"start_time":"2024-09-04T16:33:20.149499","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 2<span style=\"vertical-align: super; font-size: 60%;\">nd</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.017781,"end_time":"2024-09-04T16:33:20.211712","exception":false,"start_time":"2024-09-04T16:33:20.193931","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 2nd training dataset\ntrain_02 = fe.downsample_data(train_data, 2)","metadata":{"papermill":{"duration":0.198168,"end_time":"2024-09-04T16:33:20.427695","exception":false,"start_time":"2024-09-04T16:33:20.229527","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 2nd training dataset\npreds_02, pauc_02 = md.generate_preds(train_02, test_data, cat_cols)","metadata":{"papermill":{"duration":112.65993,"end_time":"2024-09-04T16:35:13.105848","exception":false,"start_time":"2024-09-04T16:33:20.445918","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 2nd ensemble\nresults.append((preds_02, pauc_02))","metadata":{"papermill":{"duration":0.026911,"end_time":"2024-09-04T16:35:13.152324","exception":false,"start_time":"2024-09-04T16:35:13.125413","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 3<span style=\"vertical-align: super; font-size: 60%;\">rd</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.018761,"end_time":"2024-09-04T16:35:13.190163","exception":false,"start_time":"2024-09-04T16:35:13.171402","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 3rd training dataset\ntrain_03 = fe.downsample_data(train_data, 3)","metadata":{"papermill":{"duration":0.196936,"end_time":"2024-09-04T16:35:13.406136","exception":false,"start_time":"2024-09-04T16:35:13.209200","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 3rd training dataset\npreds_03, pauc_03 = md.generate_preds(train_03, test_data, cat_cols)","metadata":{"papermill":{"duration":97.866713,"end_time":"2024-09-04T16:36:51.292163","exception":false,"start_time":"2024-09-04T16:35:13.425450","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 3rd ensemble\nresults.append((preds_03, pauc_03))","metadata":{"papermill":{"duration":0.02869,"end_time":"2024-09-04T16:36:51.341574","exception":false,"start_time":"2024-09-04T16:36:51.312884","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 4<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.020184,"end_time":"2024-09-04T16:36:51.382557","exception":false,"start_time":"2024-09-04T16:36:51.362373","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 4th training dataset\ntrain_04 = fe.downsample_data(train_data, 4)","metadata":{"papermill":{"duration":0.201111,"end_time":"2024-09-04T16:36:51.605179","exception":false,"start_time":"2024-09-04T16:36:51.404068","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 4th training dataset\npreds_04, pauc_04 = md.generate_preds(train_04, test_data, cat_cols)","metadata":{"papermill":{"duration":103.920094,"end_time":"2024-09-04T16:38:35.546064","exception":false,"start_time":"2024-09-04T16:36:51.625970","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 4th ensemble\nresults.append((preds_04, pauc_04))","metadata":{"papermill":{"duration":0.028448,"end_time":"2024-09-04T16:38:35.596220","exception":false,"start_time":"2024-09-04T16:38:35.567772","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 5<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.020561,"end_time":"2024-09-04T16:38:35.638131","exception":false,"start_time":"2024-09-04T16:38:35.617570","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 5th training dataset\ntrain_05 = fe.downsample_data(train_data, 5)","metadata":{"papermill":{"duration":0.198895,"end_time":"2024-09-04T16:38:35.857607","exception":false,"start_time":"2024-09-04T16:38:35.658712","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 5th training dataset\npreds_05, pauc_05 = md.generate_preds(train_05, test_data, cat_cols)","metadata":{"papermill":{"duration":113.106379,"end_time":"2024-09-04T16:40:28.985195","exception":false,"start_time":"2024-09-04T16:38:35.878816","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 5th ensemble\nresults.append((preds_05, pauc_05))","metadata":{"papermill":{"duration":0.028756,"end_time":"2024-09-04T16:40:29.036136","exception":false,"start_time":"2024-09-04T16:40:29.007380","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 6<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.021731,"end_time":"2024-09-04T16:40:29.079870","exception":false,"start_time":"2024-09-04T16:40:29.058139","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 6th training dataset\ntrain_06 = fe.downsample_data(train_data, 6)","metadata":{"papermill":{"duration":0.198866,"end_time":"2024-09-04T16:40:29.300590","exception":false,"start_time":"2024-09-04T16:40:29.101724","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 6th training dataset\npreds_06, pauc_06 = md.generate_preds(train_06, test_data, cat_cols)","metadata":{"papermill":{"duration":125.907909,"end_time":"2024-09-04T16:42:35.230998","exception":false,"start_time":"2024-09-04T16:40:29.323089","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 6th ensemble\nresults.append((preds_06, pauc_06))","metadata":{"papermill":{"duration":0.031036,"end_time":"2024-09-04T16:42:35.285591","exception":false,"start_time":"2024-09-04T16:42:35.254555","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 7<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.023672,"end_time":"2024-09-04T16:42:35.333037","exception":false,"start_time":"2024-09-04T16:42:35.309365","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 7th training dataset\ntrain_07 = fe.downsample_data(train_data, 7)","metadata":{"papermill":{"duration":0.210897,"end_time":"2024-09-04T16:42:35.567306","exception":false,"start_time":"2024-09-04T16:42:35.356409","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 7th training dataset\npreds_07, pauc_07 = md.generate_preds(train_07, test_data, cat_cols)","metadata":{"papermill":{"duration":113.425302,"end_time":"2024-09-04T16:44:29.016585","exception":false,"start_time":"2024-09-04T16:42:35.591283","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 7th ensemble\nresults.append((preds_07, pauc_07))","metadata":{"papermill":{"duration":0.031587,"end_time":"2024-09-04T16:44:29.072482","exception":false,"start_time":"2024-09-04T16:44:29.040895","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 8<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.023948,"end_time":"2024-09-04T16:44:29.120444","exception":false,"start_time":"2024-09-04T16:44:29.096496","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 8th training dataset\ntrain_08 = fe.downsample_data(train_data, 8)","metadata":{"papermill":{"duration":0.203752,"end_time":"2024-09-04T16:44:29.348106","exception":false,"start_time":"2024-09-04T16:44:29.144354","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 8th training dataset\npreds_08, pauc_08 = md.generate_preds(train_08, test_data, cat_cols)","metadata":{"papermill":{"duration":99.736127,"end_time":"2024-09-04T16:46:09.108488","exception":false,"start_time":"2024-09-04T16:44:29.372361","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 8th ensemble\nresults.append((preds_08, pauc_08))","metadata":{"papermill":{"duration":0.032736,"end_time":"2024-09-04T16:46:09.166783","exception":false,"start_time":"2024-09-04T16:46:09.134047","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 9<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.025144,"end_time":"2024-09-04T16:46:09.217186","exception":false,"start_time":"2024-09-04T16:46:09.192042","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 9th training dataset\ntrain_09 = fe.downsample_data(train_data, 9)","metadata":{"papermill":{"duration":0.20646,"end_time":"2024-09-04T16:46:09.448668","exception":false,"start_time":"2024-09-04T16:46:09.242208","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 9th training dataset\npreds_09, pauc_09 = md.generate_preds(train_09, test_data, cat_cols)","metadata":{"papermill":{"duration":119.868193,"end_time":"2024-09-04T16:48:09.341943","exception":false,"start_time":"2024-09-04T16:46:09.473750","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 9th ensemble\nresults.append((preds_09, pauc_09))","metadata":{"papermill":{"duration":0.033655,"end_time":"2024-09-04T16:48:09.402274","exception":false,"start_time":"2024-09-04T16:48:09.368619","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 30px; font-weight: bold;'>Develop 10<span style=\"vertical-align: super; font-size: 60%;\">th</span> ensemble</span></b>","metadata":{"papermill":{"duration":0.025313,"end_time":"2024-09-04T16:48:09.453422","exception":false,"start_time":"2024-09-04T16:48:09.428109","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Create 10th training dataset\ntrain_10 = fe.downsample_data(train_data, 10)","metadata":{"papermill":{"duration":0.206249,"end_time":"2024-09-04T16:48:09.685220","exception":false,"start_time":"2024-09-04T16:48:09.478971","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions on the test data using the 10th training dataset\npreds_10, pauc_10 = md.generate_preds(train_10, test_data, cat_cols)","metadata":{"papermill":{"duration":106.837502,"end_time":"2024-09-04T16:49:56.548966","exception":false,"start_time":"2024-09-04T16:48:09.711464","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions and validation pAUC of the 10th ensemble\nresults.append((preds_10, pauc_10))","metadata":{"papermill":{"duration":0.034381,"end_time":"2024-09-04T16:49:56.610615","exception":false,"start_time":"2024-09-04T16:49:56.576234","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><span style='color:#E0BFB8; font-size: 50px; font-weight: bold;'>Post processing</span></b>","metadata":{"papermill":{"duration":0.026622,"end_time":"2024-09-04T16:49:56.664078","exception":false,"start_time":"2024-09-04T16:49:56.637456","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Cherry pick best performing ensembles\ntop_preds = md.cherry_pick_models(results)","metadata":{"papermill":{"duration":0.034973,"end_time":"2024-09-04T16:49:56.725709","exception":false,"start_time":"2024-09-04T16:49:56.690736","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average the predictions of the best performing ensembles\npreds = np.mean(top_preds, axis=0)","metadata":{"papermill":{"duration":0.034584,"end_time":"2024-09-04T16:49:56.787120","exception":false,"start_time":"2024-09-04T16:49:56.752536","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load submission data and assign predictions to submission DataFrame\nsubm_data = pd.read_csv(CFG.subm_path)\nsubm_data['target'] = preds","metadata":{"papermill":{"duration":0.041968,"end_time":"2024-09-04T16:49:56.856542","exception":false,"start_time":"2024-09-04T16:49:56.814574","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission DataFrame\nsubm_data.to_csv('submission.csv', index=False)\ndisplay(subm_data.head())","metadata":{"papermill":{"duration":0.05111,"end_time":"2024-09-04T16:49:56.936002","exception":false,"start_time":"2024-09-04T16:49:56.884892","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}